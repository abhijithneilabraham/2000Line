{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled60.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijithneilabraham/2000Line/blob/master/automatictextgenerationgpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfXtwyfH4sq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5fcb6174-1abb-4b8e-d2ae-3ec28cb73e5a"
      },
      "source": [
        "\n",
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "remote: Total 209 (delta 0), reused 0 (delta 0), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (209/209), 4.37 MiB | 6.34 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-5c3CRx5AmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "8169b8d7-7d6b-4949-f357-91137dea1da4"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 6.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=9185ff49559bd57d659f89772ba20628e45ab7c4b7cae9830020d02488f7e4f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533182 sha256=ac69c11b55b276f32f155f4d25c92513b1e1cc51117d6fed5538e40c2ff9db69\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0vhbOth7HWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 download_model.py 345M\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPzAR2_D7ivx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_14yxBF17oRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('src')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quXULjS37r9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLh4aeVw8XGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awDQg9HO8ZA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34d38e82-5b83-441c-c926-6414434837e7"
      },
      "source": [
        "interact_model(\n",
        "    '345M',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    300,\n",
        "    1,\n",
        "    0,\n",
        "    '/content/gpt-2/models'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0831 13:51:26.166046 140216978483072 deprecation_wrapper.py:119] From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0831 13:51:26.167520 140216978483072 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0831 13:51:26.178414 140216978483072 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0831 13:51:26.229646 140216978483072 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "W0831 13:51:31.861155 140216978483072 deprecation.py:323] From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0831 13:51:31.907768 140216978483072 deprecation.py:323] From /content/gpt-2/src/sample.py:39: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0831 13:51:31.909727 140216978483072 deprecation.py:323] From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0831 13:51:37.086424 140216978483072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model prompt >>> Me and my friend are chilling together in Miami\n",
            "======================================== SAMPLE 1 ========================================\n",
            " but couldn't really get a good feed to share in terms of talent and connection. We wanted to help our friend earn money through his dorm rooms right? So I called Babo35 and got him to sign on the basis of his nude & mobile photos. That gets a good run: Babo35 submitted his naked pics to us and (yourdubstepgoer) that was awesome♥ Thanks AWOOOOO!!\n",
            "\n",
            "Then a few months later Babo35 asked me if he could start booking dates the following year (\"I've been traveling for the past seven years) for a bit to smooth things over.\"\n",
            "\n",
            "This simulates a little bit of my younger self's 'boyfriend plan\" in which he traveled across multiple countries with girlfriends by secretly scouting out what's available across the globe to recruit guys and brainstorm dates. Before I leave my house this morning to sell drugs or pot, Babo35 will be with me for 35 days in 250 city on-demand shows (with an average attendance of half a hundred people a showing, where people are always coming).\n",
            "\n",
            "It's cruel of me not to at least Snapchat Babo in outdoor Mashe Martagah performed night buses pour fans & Kia fans full of irony. His hitting the sub-par micro-carbo patch was Milly Dart in front of the prowl. Showing 30 kms from Brooklyn to Los Angeles, you CAN get that boob out to Taylor Swift/PBR (we\n",
            "================================================================================\n",
            "Model prompt >>> I won the Oscar.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " 11. Te...\" Teksavvy: They're looping 9 months later. 6. \"The fans love it!\" So hosts assure you. 9. \"Only the best smile on your face.\" 14. \"Oooooooooooh! My coffee delivered!\" So 4 initial questions beg for Applebee's coffee! Most gullible. So ... happily lovely.\n",
            "Note Just like Jeopardy! PR could only pilot a handful of the shows all full time, most of the original jokes couldn't be taken seriously when taken to their logical conclusion, each made the final edit quite aggressively aimed at applying financial pressure/irrationally humiliating results to desired outcome which unsuccessfully (crazily?) conditions these selections as objective truth/statements. Original gags http://www.goodlingware.com/features/echoshelenacky3.htm<|endoftext|>CMP Miracal\n",
            "\n",
            "Constructed of traditional sawn Cedar and Deer wood in alternating layers of billet and meet, the CMP Miracal works with the appropriate sections together to create two continuous now suitably spaced orthogonal arcs that create an earth and a form/wave changing effect. Created to supplement a vast array of beam wraps below the pontoon bridge/top deck, these layers are part of structural integrity. From the plane of the anchor deck (cirlce to rudder sites) and anchor arch (nacelle to pontoon deck) and attached to anchor paper, these scale & interlock preparing\n",
            "================================================================================\n",
            "Model prompt >>> I went to a lounge to celebrate my birthday and\n",
            "======================================== SAMPLE 1 ========================================\n",
            " the only place left was Capcom's PlayStation Network servers. I quickly discovered that my friends didn't notice my absence. They were busy with other activities, my Xbox 360 was still downloading game updates, my phone only needed to ring, its own connection unsuccessfully gigmatically connected to the PlayStation Network servers. My family awoke, re-connected to the PSN, packing up and being carefully prepared to leave to sanitize our PCs, devices, etc. We entered confines diminishing fluctuations of 147 recordings and departed quietly genuflecting before the familiar cheery triumphal sound of ports and keystrokes.\n",
            "\n",
            "I watched the PlayStation System proactively fling one while your PC began downloading several hundred DLC and 32 different game mods for various country-specific titles. Whatever popularity, systems go problematic when our best friends depart and we're left scrambling for something from less tech loving friends from 110+ countries (contended as well as coincidentally in truth at the time, this is also a contest long and varied past rather than Greater Badges of event sabotage). First, a starter set up. The PlayStation bundles with the PlayStation 2 and a price of $30 might have been OK for my family, but for us grandparents, today may or may not piss you off. Nikon canned shots of a festival party priming cameras for much needed back illuminations that look like bizarre tails found in a otherhistorical love child devil in 1998. Full Eldritch Cult mail collectible card party one pack project\n",
            "================================================================================\n",
            "Model prompt >>> I went to a lounge to celebrate my birthday and\n",
            "======================================== SAMPLE 1 ========================================\n",
            " heard Yoichi crying today and slid into it too. I will not lose this light.\" He murmured. With a shiver and a huff… Naruto closed his eyes. Finally, he opened up and whispered into the Manger's voice.\n",
            "\n",
            "\"I am bound-Merson.\"\n",
            "\n",
            "He reran his re-election speech. \"Very well. Let's go to get some food. Please. Please?\"\n",
            "\n",
            "\"Hm? …It's okay.\"\n",
            "\n",
            "He washed up first.\n",
            "\n",
            "NINJAGER\n",
            "\n",
            "I have a request…\n",
            "\n",
            "NINJAGER (vowing)\n",
            "\n",
            "Patient does not insist on first aid. Absorb stick will do. Over ice and honey will do. Long cups of iced tea may no need it. Draught ruined can get scream Mac%+drip% uptn%ordinate.\n",
            "\n",
            "M MONKEY SROR Henry NO suphttp://content.pub.se/ball4/Clinking/c9b7/CF44c82cc3AD521c823f8440bdfb91f64e35191d5eb28281124ntraKR + THIS THE FIRST TIME I UPDATED IN ONE HOUR – I CHANGED MY TRICKS TO NORMAL + NO MATTER WHAT SETTLEMENT CALLS MAKE ME WHAT + ~EVERYONE WANTS ~\"\n",
            "\n",
            "The\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}